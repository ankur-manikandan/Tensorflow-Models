{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])\n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes_dataset = load_diabetes()\n",
    "print(diabetes_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables to store the data and the targets\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.hist(targets, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the target data (this will make clearer training curves)\n",
    "targets = (targets - targets.mean(axis=0)) / targets.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.hist(targets, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A customer callback defined for the training process\n",
    "class TrainingCallback(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Training: starting batch {batch}\")\n",
    "              \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f\"Training: finished batch {batch}\")\n",
    "              \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "              \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.3 # dropout rate\n",
    "\n",
    "# Feedforward model with batch norm layers\n",
    "model = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_shape=(train_data.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100\n",
      "Starting epoch 0\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 4s - loss: 3.5759Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 2s 134ms/step - loss: 3.4286 - val_loss: 0.8621\n",
      "Epoch 2/100\n",
      "Starting epoch 1\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3347Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.4655 - val_loss: 0.8661\n",
      "Epoch 3/100\n",
      "Starting epoch 2\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1547Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.3528 - val_loss: 0.8680\n",
      "Epoch 4/100\n",
      "Starting epoch 3\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.4913Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3229 - val_loss: 0.8678\n",
      "Epoch 5/100\n",
      "Starting epoch 4\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.3166Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1426 - val_loss: 0.8639\n",
      "Epoch 6/100\n",
      "Starting epoch 5\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5118Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7030 - val_loss: 0.8642\n",
      "Epoch 7/100\n",
      "Starting epoch 6\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9528Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.7028 - val_loss: 0.8602\n",
      "Epoch 8/100\n",
      "Starting epoch 7\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7305Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5257 - val_loss: 0.8560\n",
      "Epoch 9/100\n",
      "Starting epoch 8\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1769Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0751 - val_loss: 0.8547\n",
      "Epoch 10/100\n",
      "Starting epoch 9\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6756Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7873 - val_loss: 0.8551\n",
      "Epoch 11/100\n",
      "Starting epoch 10\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6944Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6026 - val_loss: 0.8567\n",
      "Epoch 12/100\n",
      "Starting epoch 11\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3087Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5650 - val_loss: 0.8576\n",
      "Epoch 13/100\n",
      "Starting epoch 12\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.1422Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8608 - val_loss: 0.8609\n",
      "Epoch 14/100\n",
      "Starting epoch 13\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6076Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6231 - val_loss: 0.8655\n",
      "Epoch 15/100\n",
      "Starting epoch 14\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4354Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5240 - val_loss: 0.8697\n",
      "Epoch 16/100\n",
      "Starting epoch 15\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.7515Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.4753 - val_loss: 0.8610\n",
      "Epoch 17/100\n",
      "Starting epoch 16\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5316Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5144 - val_loss: 0.8531\n",
      "Epoch 18/100\n",
      "Starting epoch 17\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.6208Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.4088 - val_loss: 0.8546\n",
      "Epoch 19/100\n",
      "Starting epoch 18\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5248Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5370 - val_loss: 0.8573\n",
      "Epoch 20/100\n",
      "Starting epoch 19\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7812Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0167 - val_loss: 0.8672\n",
      "Epoch 21/100\n",
      "Starting epoch 20\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1045Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1632 - val_loss: 0.8702\n",
      "Epoch 22/100\n",
      "Starting epoch 21\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.3943Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3182 - val_loss: 0.8675\n",
      "Epoch 23/100\n",
      "Starting epoch 22\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0529Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2030 - val_loss: 0.8589\n",
      "Epoch 24/100\n",
      "Starting epoch 23\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6379Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8468 - val_loss: 0.8631\n",
      "Epoch 25/100\n",
      "Starting epoch 24\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2352Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2147 - val_loss: 0.8526\n",
      "Epoch 26/100\n",
      "Starting epoch 25\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0677Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0971 - val_loss: 0.8343\n",
      "Epoch 27/100\n",
      "Starting epoch 26\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9188Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1175 - val_loss: 0.8198\n",
      "Epoch 28/100\n",
      "Starting epoch 27\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2323Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1857 - val_loss: 0.8145\n",
      "Epoch 29/100\n",
      "Starting epoch 28\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.4836Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1286 - val_loss: 0.8187\n",
      "Epoch 30/100\n",
      "Starting epoch 29\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1945Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0952 - val_loss: 0.8327\n",
      "Epoch 31/100\n",
      "Starting epoch 30\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.5748Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2658 - val_loss: 0.8476\n",
      "Epoch 32/100\n",
      "Starting epoch 31\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9343Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9521 - val_loss: 0.8503\n",
      "Epoch 33/100\n",
      "Starting epoch 32\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1063Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0471 - val_loss: 0.8632\n",
      "Epoch 34/100\n",
      "Starting epoch 33\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9693Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9378 - val_loss: 0.8677\n",
      "Epoch 35/100\n",
      "Starting epoch 34\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9821Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9709 - val_loss: 0.8661\n",
      "Epoch 36/100\n",
      "Starting epoch 35\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1101Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9857 - val_loss: 0.8821\n",
      "Epoch 37/100\n",
      "Starting epoch 36\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0672Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1487 - val_loss: 0.8752\n",
      "Epoch 38/100\n",
      "Starting epoch 37\n",
      "Training: starting batch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.2722Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0636 - val_loss: 0.8669\n",
      "Epoch 39/100\n",
      "Starting epoch 38\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.0840Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9424 - val_loss: 0.8657\n",
      "Epoch 40/100\n",
      "Starting epoch 39\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8891Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8282 - val_loss: 0.8844\n",
      "Epoch 41/100\n",
      "Starting epoch 40\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9203Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9138 - val_loss: 0.8817\n",
      "Epoch 42/100\n",
      "Starting epoch 41\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7691Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.8206 - val_loss: 0.8698\n",
      "Epoch 43/100\n",
      "Starting epoch 42\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7783Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7990 - val_loss: 0.8878\n",
      "Epoch 44/100\n",
      "Starting epoch 43\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7715Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8575 - val_loss: 0.8838\n",
      "Epoch 45/100\n",
      "Starting epoch 44\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1.1097Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9060 - val_loss: 0.8981\n",
      "Epoch 46/100\n",
      "Starting epoch 45\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6985Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8111 - val_loss: 0.9149\n",
      "Epoch 47/100\n",
      "Starting epoch 46\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7131Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7464 - val_loss: 0.9298\n",
      "Epoch 48/100\n",
      "Starting epoch 47\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8750Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.8747 - val_loss: 0.9168\n",
      "Epoch 49/100\n",
      "Starting epoch 48\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.9019Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8898 - val_loss: 0.9037\n",
      "Epoch 50/100\n",
      "Starting epoch 49\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6737Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7580 - val_loss: 0.8803\n",
      "Epoch 51/100\n",
      "Starting epoch 50\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8266Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7775 - val_loss: 0.8642\n",
      "Epoch 52/100\n",
      "Starting epoch 51\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5993Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7399 - val_loss: 0.8677\n",
      "Epoch 53/100\n",
      "Starting epoch 52\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5652Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7502 - val_loss: 0.8776\n",
      "Epoch 54/100\n",
      "Starting epoch 53\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8443Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8241 - val_loss: 0.8882\n",
      "Epoch 55/100\n",
      "Starting epoch 54\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7399Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7435 - val_loss: 0.8901\n",
      "Epoch 56/100\n",
      "Starting epoch 55\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7948Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7907 - val_loss: 0.8742\n",
      "Epoch 57/100\n",
      "Starting epoch 56\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8412Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8214 - val_loss: 0.8440\n",
      "Epoch 58/100\n",
      "Starting epoch 57\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7406Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7135 - val_loss: 0.8416\n",
      "Epoch 59/100\n",
      "Starting epoch 58\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7412Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7355 - val_loss: 0.8462\n",
      "Epoch 60/100\n",
      "Starting epoch 59\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8993Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7694 - val_loss: 0.8589\n",
      "Epoch 61/100\n",
      "Starting epoch 60\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6423Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6828 - val_loss: 0.8616\n",
      "Epoch 62/100\n",
      "Starting epoch 61\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7209Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6741 - val_loss: 0.8558\n",
      "Epoch 63/100\n",
      "Starting epoch 62\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6186Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6806 - val_loss: 0.8826\n",
      "Epoch 64/100\n",
      "Starting epoch 63\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6744Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6516 - val_loss: 0.8870\n",
      "Epoch 65/100\n",
      "Starting epoch 64\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7409Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6698 - val_loss: 0.8526\n",
      "Epoch 66/100\n",
      "Starting epoch 65\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7354Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6439 - val_loss: 0.8379\n",
      "Epoch 67/100\n",
      "Starting epoch 66\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5932Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6901 - val_loss: 0.8113\n",
      "Epoch 68/100\n",
      "Starting epoch 67\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.8749Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7310 - val_loss: 0.7673\n",
      "Epoch 69/100\n",
      "Starting epoch 68\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5985Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6465 - val_loss: 0.7301\n",
      "Epoch 70/100\n",
      "Starting epoch 69\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5593Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5800 - val_loss: 0.7007\n",
      "Epoch 71/100\n",
      "Starting epoch 70\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4455Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5178 - val_loss: 0.6844\n",
      "Epoch 72/100\n",
      "Starting epoch 71\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4609Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5637 - val_loss: 0.6778\n",
      "Epoch 73/100\n",
      "Starting epoch 72\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6579Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6404 - val_loss: 0.6704\n",
      "Epoch 74/100\n",
      "Starting epoch 73\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6396Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6357 - val_loss: 0.6717\n",
      "Epoch 75/100\n",
      "Starting epoch 74\n",
      "Training: starting batch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5993Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6442 - val_loss: 0.6722\n",
      "Epoch 76/100\n",
      "Starting epoch 75\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5495Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6049 - val_loss: 0.6795\n",
      "Epoch 77/100\n",
      "Starting epoch 76\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7657Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6715 - val_loss: 0.6999\n",
      "Epoch 78/100\n",
      "Starting epoch 77\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4319Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5940 - val_loss: 0.7048\n",
      "Epoch 79/100\n",
      "Starting epoch 78\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6592Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5592 - val_loss: 0.7199\n",
      "Epoch 80/100\n",
      "Starting epoch 79\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6886Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6069 - val_loss: 0.7285\n",
      "Epoch 81/100\n",
      "Starting epoch 80\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6475Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5802 - val_loss: 0.7351\n",
      "Epoch 82/100\n",
      "Starting epoch 81\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5464Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5414 - val_loss: 0.7369\n",
      "Epoch 83/100\n",
      "Starting epoch 82\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4877Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5650 - val_loss: 0.7414\n",
      "Epoch 84/100\n",
      "Starting epoch 83\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6250Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5944 - val_loss: 0.7204\n",
      "Epoch 85/100\n",
      "Starting epoch 84\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5734Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5941 - val_loss: 0.7119\n",
      "Epoch 86/100\n",
      "Starting epoch 85\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6761Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6585 - val_loss: 0.6899\n",
      "Epoch 87/100\n",
      "Starting epoch 86\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6382Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6039 - val_loss: 0.6669\n",
      "Epoch 88/100\n",
      "Starting epoch 87\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5711Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5494 - val_loss: 0.6578\n",
      "Epoch 89/100\n",
      "Starting epoch 88\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.7377Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5998 - val_loss: 0.6450\n",
      "Epoch 90/100\n",
      "Starting epoch 89\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5061Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5079 - val_loss: 0.6247\n",
      "Epoch 91/100\n",
      "Starting epoch 90\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4598Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5245 - val_loss: 0.6096\n",
      "Epoch 92/100\n",
      "Starting epoch 91\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5699Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5894 - val_loss: 0.5940\n",
      "Epoch 93/100\n",
      "Starting epoch 92\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4194Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5349 - val_loss: 0.5922\n",
      "Epoch 94/100\n",
      "Starting epoch 93\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4511Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5226 - val_loss: 0.5957\n",
      "Epoch 95/100\n",
      "Starting epoch 94\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4857Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5370 - val_loss: 0.5999\n",
      "Epoch 96/100\n",
      "Starting epoch 95\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.4872Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5234 - val_loss: 0.6096\n",
      "Epoch 97/100\n",
      "Starting epoch 96\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5112Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4857 - val_loss: 0.6196\n",
      "Epoch 98/100\n",
      "Starting epoch 97\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5207Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5302 - val_loss: 0.6154\n",
      "Epoch 99/100\n",
      "Starting epoch 98\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5076Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5368 - val_loss: 0.5916\n",
      "Epoch 100/100\n",
      "Starting epoch 99\n",
      "Training: starting batch 0\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5925Training: finished batch 0\n",
      "Training: starting batch 1\n",
      "Training: finished batch 1\n",
      "Training: starting batch 2\n",
      "Training: finished batch 2\n",
      "Training: starting batch 3\n",
      "Training: finished batch 3\n",
      "Training: starting batch 4\n",
      "Training: finished batch 4\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5707 - val_loss: 0.5773\n",
      "Finished training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa13341630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(train_data, train_targets, epochs=100,\n",
    "         batch_size=64, validation_split=0.2, callbacks=[TrainingCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also define a callback during the evaluation process\n",
    "class TestingCallback(Callback):\n",
    "    \n",
    "    def on_test_begin(self, logs=None):\n",
    "        print(\"Starting testing...\")\n",
    "        \n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Testing: starting batch {batch}\")\n",
    "              \n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(f\"Testing: finished batch {batch}\")\n",
    "              \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "              \n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"Finished testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Testing: starting batch 0\n",
      "Testing: finished batch 0\n",
      "Testing: starting batch 1\n",
      "Testing: finished batch 1\n",
      "2/2 - 0s - loss: 0.8204\n",
      "Finished testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8204417824745178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_data, test_targets, verbose=2, callbacks=[TestingCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly we can also create a callback for model predictions\n",
    "class PredictionCallback(Callback):\n",
    "    \n",
    "    def on_predict_begin(self, logs=None):\n",
    "        print(\"Starting predicting...\")\n",
    "        \n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        print(f\"Predicting: starting batch {batch}\")\n",
    "              \n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print(f\"Predicting: finished batch {batch}\")\n",
    "              \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "              \n",
    "    def on_predict_end(self, logs=None):\n",
    "        print(\"Finished predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "Predicting: starting batch 0\n",
      "Predicting: finished batch 0\n",
      "Predicting: starting batch 1\n",
      "Predicting: finished batch 1\n",
      "2/2 - 0s\n",
      "Finished predicting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.39093262],\n",
       "       [-0.7862673 ],\n",
       "       [-0.6887794 ],\n",
       "       [ 0.05878257],\n",
       "       [ 0.03196519],\n",
       "       [-0.7498262 ],\n",
       "       [ 0.22862847],\n",
       "       [-0.5055863 ],\n",
       "       [-0.48353332],\n",
       "       [-0.5637872 ],\n",
       "       [-0.60679865],\n",
       "       [-0.760397  ],\n",
       "       [-0.8327871 ],\n",
       "       [-0.820203  ],\n",
       "       [-0.66991836],\n",
       "       [-0.9217755 ],\n",
       "       [-0.6854082 ],\n",
       "       [-0.13775876],\n",
       "       [-0.7417716 ],\n",
       "       [-0.8288577 ],\n",
       "       [-0.5369203 ],\n",
       "       [ 0.83554405],\n",
       "       [-0.80999416],\n",
       "       [-0.7676277 ],\n",
       "       [-0.12848942],\n",
       "       [-0.35717368],\n",
       "       [ 0.5198148 ],\n",
       "       [-0.6307942 ],\n",
       "       [-0.39294076],\n",
       "       [-0.24035755],\n",
       "       [ 0.1145458 ],\n",
       "       [ 0.01007382],\n",
       "       [-0.6193914 ],\n",
       "       [-0.32637745],\n",
       "       [-0.5696589 ],\n",
       "       [-0.4419432 ],\n",
       "       [-0.8060169 ],\n",
       "       [-0.360767  ],\n",
       "       [-0.1372452 ],\n",
       "       [-0.7759735 ],\n",
       "       [-0.5593819 ],\n",
       "       [-0.47432745],\n",
       "       [-0.66692   ],\n",
       "       [-0.45762238],\n",
       "       [-0.89295447]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "model.predict(test_data, callbacks=[PredictionCallback()], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to access the loss and metric values of the model\n",
    "class LossAndMetricCallback(Callback):\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 2 == 0:\n",
    "            print(\"\\n After batch {}, the loss is {:7.2f}.\".format(batch, logs['loss']))\n",
    "            \n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"\\n After batch {}, the loss is {:7.2f}.\".format(batch, logs['loss']))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs['loss'], logs['mae']))\n",
    "        \n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/7 [===>..........................] - ETA: 5s - loss: 0.5216 - mae: 0.5850\n",
      " After batch 0, the loss is    0.52.\n",
      "\n",
      " After batch 2, the loss is    0.61.\n",
      "\n",
      " After batch 4, the loss is    0.60.\n",
      "\n",
      " After batch 6, the loss is    0.57.\n",
      "7/7 [==============================] - 1s 3ms/step - loss: 0.5818 - mae: 0.6182\n",
      "Epoch 0: Average loss is    0.57, mean absolute error is    0.61.\n",
      "Epoch 2/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4489 - mae: 0.5498\n",
      " After batch 0, the loss is    0.45.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.52.\n",
      "\n",
      " After batch 6, the loss is    0.51.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5058 - mae: 0.5720\n",
      "Epoch 1: Average loss is    0.51, mean absolute error is    0.57.\n",
      "Epoch 3/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.6373 - mae: 0.6613\n",
      " After batch 0, the loss is    0.64.\n",
      "\n",
      " After batch 2, the loss is    0.49.\n",
      "\n",
      " After batch 4, the loss is    0.52.\n",
      "\n",
      " After batch 6, the loss is    0.56.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5421 - mae: 0.5918\n",
      "Epoch 2: Average loss is    0.56, mean absolute error is    0.60.\n",
      "Epoch 4/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4678 - mae: 0.5628\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.55.\n",
      "\n",
      " After batch 4, the loss is    0.54.\n",
      "\n",
      " After batch 6, the loss is    0.54.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5283 - mae: 0.5813\n",
      "Epoch 3: Average loss is    0.54, mean absolute error is    0.59.\n",
      "Epoch 5/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4449 - mae: 0.5314\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.51.\n",
      "\n",
      " After batch 4, the loss is    0.57.\n",
      "\n",
      " After batch 6, the loss is    0.57.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5365 - mae: 0.5933\n",
      "Epoch 4: Average loss is    0.57, mean absolute error is    0.62.\n",
      "Epoch 6/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5473 - mae: 0.5792\n",
      " After batch 0, the loss is    0.55.\n",
      "\n",
      " After batch 2, the loss is    0.53.\n",
      "\n",
      " After batch 4, the loss is    0.53.\n",
      "\n",
      " After batch 6, the loss is    0.56.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5482 - mae: 0.5850\n",
      "Epoch 5: Average loss is    0.56, mean absolute error is    0.59.\n",
      "Epoch 7/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5050 - mae: 0.5550\n",
      " After batch 0, the loss is    0.51.\n",
      "\n",
      " After batch 2, the loss is    0.50.\n",
      "\n",
      " After batch 4, the loss is    0.50.\n",
      "\n",
      " After batch 6, the loss is    0.51.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4974 - mae: 0.5655\n",
      "Epoch 6: Average loss is    0.51, mean absolute error is    0.57.\n",
      "Epoch 8/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5273 - mae: 0.5725\n",
      " After batch 0, the loss is    0.53.\n",
      "\n",
      " After batch 2, the loss is    0.52.\n",
      "\n",
      " After batch 4, the loss is    0.51.\n",
      "\n",
      " After batch 6, the loss is    0.53.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5206 - mae: 0.5822\n",
      "Epoch 7: Average loss is    0.53, mean absolute error is    0.59.\n",
      "Epoch 9/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4281 - mae: 0.5498\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.47.\n",
      "\n",
      " After batch 4, the loss is    0.45.\n",
      "\n",
      " After batch 6, the loss is    0.47.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4604 - mae: 0.5559\n",
      "Epoch 8: Average loss is    0.47, mean absolute error is    0.56.\n",
      "Epoch 10/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4652 - mae: 0.5541\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.49.\n",
      "\n",
      " After batch 6, the loss is    0.49.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4818 - mae: 0.5658\n",
      "Epoch 9: Average loss is    0.49, mean absolute error is    0.57.\n",
      "Epoch 11/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5617 - mae: 0.5783\n",
      " After batch 0, the loss is    0.56.\n",
      "\n",
      " After batch 2, the loss is    0.49.\n",
      "\n",
      " After batch 4, the loss is    0.49.\n",
      "\n",
      " After batch 6, the loss is    0.51.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5121 - mae: 0.5671\n",
      "Epoch 10: Average loss is    0.51, mean absolute error is    0.57.\n",
      "Epoch 12/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3761 - mae: 0.4851\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.49.\n",
      "\n",
      " After batch 4, the loss is    0.50.\n",
      "\n",
      " After batch 6, the loss is    0.50.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4810 - mae: 0.5511\n",
      "Epoch 11: Average loss is    0.50, mean absolute error is    0.57.\n",
      "Epoch 13/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3798 - mae: 0.4920\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.47.\n",
      "\n",
      " After batch 6, the loss is    0.49.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4410 - mae: 0.5248\n",
      "Epoch 12: Average loss is    0.49, mean absolute error is    0.56.\n",
      "Epoch 14/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5621 - mae: 0.6037\n",
      " After batch 0, the loss is    0.56.\n",
      "\n",
      " After batch 2, the loss is    0.49.\n",
      "\n",
      " After batch 4, the loss is    0.50.\n",
      "\n",
      " After batch 6, the loss is    0.49.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5055 - mae: 0.5748\n",
      "Epoch 13: Average loss is    0.49, mean absolute error is    0.57.\n",
      "Epoch 15/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4133 - mae: 0.5287\n",
      " After batch 0, the loss is    0.41.\n",
      "\n",
      " After batch 2, the loss is    0.61.\n",
      "\n",
      " After batch 4, the loss is    0.53.\n",
      "\n",
      " After batch 6, the loss is    0.52.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5327 - mae: 0.5895\n",
      "Epoch 14: Average loss is    0.52, mean absolute error is    0.58.\n",
      "Epoch 16/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5146 - mae: 0.5466\n",
      " After batch 0, the loss is    0.51.\n",
      "\n",
      " After batch 2, the loss is    0.50.\n",
      "\n",
      " After batch 4, the loss is    0.50.\n",
      "\n",
      " After batch 6, the loss is    0.48.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5018 - mae: 0.5697\n",
      "Epoch 15: Average loss is    0.48, mean absolute error is    0.57.\n",
      "Epoch 17/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4855 - mae: 0.5673\n",
      " After batch 0, the loss is    0.49.\n",
      "\n",
      " After batch 2, the loss is    0.50.\n",
      "\n",
      " After batch 4, the loss is    0.49.\n",
      "\n",
      " After batch 6, the loss is    0.47.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4864 - mae: 0.5641\n",
      "Epoch 16: Average loss is    0.47, mean absolute error is    0.56.\n",
      "Epoch 18/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3955 - mae: 0.5166\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.52.\n",
      "\n",
      " After batch 4, the loss is    0.50.\n",
      "\n",
      " After batch 6, the loss is    0.48.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4797 - mae: 0.5554\n",
      "Epoch 17: Average loss is    0.48, mean absolute error is    0.56.\n",
      "Epoch 19/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3809 - mae: 0.4983\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.47.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4529 - mae: 0.5466\n",
      "Epoch 18: Average loss is    0.47, mean absolute error is    0.56.\n",
      "Epoch 20/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3427 - mae: 0.4668\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.47.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.47.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4432 - mae: 0.5379\n",
      "Epoch 19: Average loss is    0.47, mean absolute error is    0.55.\n",
      "Epoch 21/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4382 - mae: 0.5084\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.50.\n",
      "\n",
      " After batch 4, the loss is    0.54.\n",
      "\n",
      " After batch 6, the loss is    0.55.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5086 - mae: 0.5671\n",
      "Epoch 20: Average loss is    0.55, mean absolute error is    0.59.\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3851 - mae: 0.5064\n",
      " After batch 0, the loss is    0.39.\n",
      "\n",
      " After batch 2, the loss is    0.43.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.47.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4360 - mae: 0.5433\n",
      "Epoch 21: Average loss is    0.47, mean absolute error is    0.56.\n",
      "Epoch 23/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3994 - mae: 0.5210\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.53.\n",
      "\n",
      " After batch 4, the loss is    0.49.\n",
      "\n",
      " After batch 6, the loss is    0.49.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4819 - mae: 0.5617\n",
      "Epoch 22: Average loss is    0.49, mean absolute error is    0.56.\n",
      "Epoch 24/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4312 - mae: 0.5285\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.47.\n",
      "\n",
      " After batch 6, the loss is    0.48.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4518 - mae: 0.5423\n",
      "Epoch 23: Average loss is    0.48, mean absolute error is    0.56.\n",
      "Epoch 25/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4589 - mae: 0.4980\n",
      " After batch 0, the loss is    0.46.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.48.\n",
      "\n",
      " After batch 6, the loss is    0.48.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4780 - mae: 0.5390\n",
      "Epoch 24: Average loss is    0.48, mean absolute error is    0.55.\n",
      "Epoch 26/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3858 - mae: 0.5087\n",
      " After batch 0, the loss is    0.39.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4244 - mae: 0.5286\n",
      "Epoch 25: Average loss is    0.45, mean absolute error is    0.54.\n",
      "Epoch 27/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3799 - mae: 0.4973\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.38.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4088 - mae: 0.5129\n",
      "Epoch 26: Average loss is    0.43, mean absolute error is    0.52.\n",
      "Epoch 28/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4464 - mae: 0.5685\n",
      " After batch 0, the loss is    0.45.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.42.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4334 - mae: 0.5401\n",
      "Epoch 27: Average loss is    0.44, mean absolute error is    0.54.\n",
      "Epoch 29/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5774 - mae: 0.6256\n",
      " After batch 0, the loss is    0.58.\n",
      "\n",
      " After batch 2, the loss is    0.47.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.46.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4847 - mae: 0.5656\n",
      "Epoch 28: Average loss is    0.46, mean absolute error is    0.55.\n",
      "Epoch 30/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3836 - mae: 0.4890\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.46.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4374 - mae: 0.5269\n",
      "Epoch 29: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 31/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3710 - mae: 0.4963\n",
      " After batch 0, the loss is    0.37.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4245 - mae: 0.5225\n",
      "Epoch 30: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 32/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3344 - mae: 0.4785\n",
      " After batch 0, the loss is    0.33.\n",
      "\n",
      " After batch 2, the loss is    0.40.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4122 - mae: 0.5193\n",
      "Epoch 31: Average loss is    0.43, mean absolute error is    0.52.\n",
      "Epoch 33/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4728 - mae: 0.5184\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.45.\n",
      "\n",
      " After batch 6, the loss is    0.46.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4492 - mae: 0.5270\n",
      "Epoch 32: Average loss is    0.46, mean absolute error is    0.54.\n",
      "Epoch 34/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4074 - mae: 0.5186\n",
      " After batch 0, the loss is    0.41.\n",
      "\n",
      " After batch 2, the loss is    0.35.\n",
      "\n",
      " After batch 4, the loss is    0.37.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3870 - mae: 0.5046\n",
      "Epoch 33: Average loss is    0.43, mean absolute error is    0.53.\n",
      "Epoch 35/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3812 - mae: 0.5220\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.43.\n",
      "\n",
      " After batch 4, the loss is    0.41.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4203 - mae: 0.5257\n",
      "Epoch 34: Average loss is    0.44, mean absolute error is    0.54.\n",
      "Epoch 36/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5392 - mae: 0.6297\n",
      " After batch 0, the loss is    0.54.\n",
      "\n",
      " After batch 2, the loss is    0.52.\n",
      "\n",
      " After batch 4, the loss is    0.48.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4942 - mae: 0.5789\n",
      "Epoch 35: Average loss is    0.45, mean absolute error is    0.55.\n",
      "Epoch 37/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4711 - mae: 0.5658\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.47.\n",
      "\n",
      " After batch 4, the loss is    0.49.\n",
      "\n",
      " After batch 6, the loss is    0.46.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4717 - mae: 0.5560\n",
      "Epoch 36: Average loss is    0.46, mean absolute error is    0.55.\n",
      "Epoch 38/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4080 - mae: 0.5174\n",
      " After batch 0, the loss is    0.41.\n",
      "\n",
      " After batch 2, the loss is    0.44.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4400 - mae: 0.5413\n",
      "Epoch 37: Average loss is    0.45, mean absolute error is    0.55.\n",
      "Epoch 39/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4697 - mae: 0.5446\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.49.\n",
      "\n",
      " After batch 4, the loss is    0.47.\n",
      "\n",
      " After batch 6, the loss is    0.48.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4787 - mae: 0.5529\n",
      "Epoch 38: Average loss is    0.48, mean absolute error is    0.55.\n",
      "Epoch 40/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5264 - mae: 0.6081\n",
      " After batch 0, the loss is    0.53.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4633 - mae: 0.5553\n",
      "Epoch 39: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 41/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3474 - mae: 0.4863\n",
      " After batch 0, the loss is    0.35.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.42.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4207 - mae: 0.5141\n",
      "Epoch 40: Average loss is    0.42, mean absolute error is    0.51.\n",
      "Epoch 42/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3379 - mae: 0.4660\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.36.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.42.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3846 - mae: 0.4858\n",
      "Epoch 41: Average loss is    0.42, mean absolute error is    0.50.\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4608 - mae: 0.5280\n",
      " After batch 0, the loss is    0.46.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.47.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4516 - mae: 0.5261\n",
      "Epoch 42: Average loss is    0.45, mean absolute error is    0.53.\n",
      "Epoch 44/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3800 - mae: 0.5051\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.44.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4210 - mae: 0.5250\n",
      "Epoch 43: Average loss is    0.45, mean absolute error is    0.54.\n",
      "Epoch 45/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5304 - mae: 0.6053\n",
      " After batch 0, the loss is    0.53.\n",
      "\n",
      " After batch 2, the loss is    0.43.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4443 - mae: 0.5360\n",
      "Epoch 44: Average loss is    0.45, mean absolute error is    0.53.\n",
      "Epoch 46/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4037 - mae: 0.5181\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.44.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4319 - mae: 0.5285\n",
      "Epoch 45: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 47/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3618 - mae: 0.4877\n",
      " After batch 0, the loss is    0.36.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.46.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4361 - mae: 0.5321\n",
      "Epoch 46: Average loss is    0.45, mean absolute error is    0.55.\n",
      "Epoch 48/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4741 - mae: 0.5447\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.50.\n",
      "\n",
      " After batch 4, the loss is    0.47.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4613 - mae: 0.5515\n",
      "Epoch 47: Average loss is    0.43, mean absolute error is    0.53.\n",
      "Epoch 49/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4214 - mae: 0.5413\n",
      " After batch 0, the loss is    0.42.\n",
      "\n",
      " After batch 2, the loss is    0.46.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4334 - mae: 0.5293\n",
      "Epoch 48: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 50/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4845 - mae: 0.5491\n",
      " After batch 0, the loss is    0.48.\n",
      "\n",
      " After batch 2, the loss is    0.46.\n",
      "\n",
      " After batch 4, the loss is    0.45.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4528 - mae: 0.5372\n",
      "Epoch 49: Average loss is    0.45, mean absolute error is    0.54.\n",
      "Epoch 51/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3742 - mae: 0.5204\n",
      " After batch 0, the loss is    0.37.\n",
      "\n",
      " After batch 2, the loss is    0.38.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3838 - mae: 0.5088\n",
      "Epoch 50: Average loss is    0.39, mean absolute error is    0.51.\n",
      "Epoch 52/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4419 - mae: 0.5230\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.44.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4361 - mae: 0.5183\n",
      "Epoch 51: Average loss is    0.43, mean absolute error is    0.52.\n",
      "Epoch 53/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4306 - mae: 0.5108\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.48.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4407 - mae: 0.5240\n",
      "Epoch 52: Average loss is    0.45, mean absolute error is    0.53.\n",
      "Epoch 54/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4382 - mae: 0.5372\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.44.\n",
      "\n",
      " After batch 4, the loss is    0.42.\n",
      "\n",
      " After batch 6, the loss is    0.45.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4425 - mae: 0.5431\n",
      "Epoch 53: Average loss is    0.45, mean absolute error is    0.55.\n",
      "Epoch 55/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3036 - mae: 0.4515\n",
      " After batch 0, the loss is    0.30.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.41.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3855 - mae: 0.5077\n",
      "Epoch 54: Average loss is    0.41, mean absolute error is    0.52.\n",
      "Epoch 56/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3520 - mae: 0.4773\n",
      " After batch 0, the loss is    0.35.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3821 - mae: 0.4985\n",
      "Epoch 55: Average loss is    0.41, mean absolute error is    0.52.\n",
      "Epoch 57/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4729 - mae: 0.5625\n",
      " After batch 0, the loss is    0.47.\n",
      "\n",
      " After batch 2, the loss is    0.45.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4346 - mae: 0.5266\n",
      "Epoch 56: Average loss is    0.41, mean absolute error is    0.51.\n",
      "Epoch 58/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4131 - mae: 0.5191\n",
      " After batch 0, the loss is    0.41.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4001 - mae: 0.5083\n",
      "Epoch 57: Average loss is    0.41, mean absolute error is    0.51.\n",
      "Epoch 59/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3781 - mae: 0.4669\n",
      " After batch 0, the loss is    0.38.\n",
      "\n",
      " After batch 2, the loss is    0.37.\n",
      "\n",
      " After batch 4, the loss is    0.41.\n",
      "\n",
      " After batch 6, the loss is    0.42.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4014 - mae: 0.5022\n",
      "Epoch 58: Average loss is    0.42, mean absolute error is    0.52.\n",
      "Epoch 60/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2705 - mae: 0.4257\n",
      " After batch 0, the loss is    0.27.\n",
      "\n",
      " After batch 2, the loss is    0.40.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3707 - mae: 0.4918\n",
      "Epoch 59: Average loss is    0.38, mean absolute error is    0.50.\n",
      "Epoch 61/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3934 - mae: 0.5131\n",
      " After batch 0, the loss is    0.39.\n",
      "\n",
      " After batch 2, the loss is    0.40.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4011 - mae: 0.5110\n",
      "Epoch 60: Average loss is    0.41, mean absolute error is    0.52.\n",
      "Epoch 62/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4433 - mae: 0.5412\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.42.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4161 - mae: 0.5111\n",
      "Epoch 61: Average loss is    0.42, mean absolute error is    0.51.\n",
      "Epoch 63/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4351 - mae: 0.5296\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3967 - mae: 0.5051\n",
      "Epoch 62: Average loss is    0.39, mean absolute error is    0.51.\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3396 - mae: 0.4621\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.42.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.42.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3999 - mae: 0.5060\n",
      "Epoch 63: Average loss is    0.42, mean absolute error is    0.52.\n",
      "Epoch 65/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5559 - mae: 0.5792\n",
      " After batch 0, the loss is    0.56.\n",
      "\n",
      " After batch 2, the loss is    0.45.\n",
      "\n",
      " After batch 4, the loss is    0.45.\n",
      "\n",
      " After batch 6, the loss is    0.43.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4545 - mae: 0.5345\n",
      "Epoch 64: Average loss is    0.43, mean absolute error is    0.53.\n",
      "Epoch 66/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3506 - mae: 0.4662\n",
      " After batch 0, the loss is    0.35.\n",
      "\n",
      " After batch 2, the loss is    0.37.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3730 - mae: 0.4925\n",
      "Epoch 65: Average loss is    0.38, mean absolute error is    0.49.\n",
      "Epoch 67/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2936 - mae: 0.4310\n",
      " After batch 0, the loss is    0.29.\n",
      "\n",
      " After batch 2, the loss is    0.36.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3671 - mae: 0.4811\n",
      "Epoch 66: Average loss is    0.39, mean absolute error is    0.50.\n",
      "Epoch 68/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4803 - mae: 0.5583\n",
      " After batch 0, the loss is    0.48.\n",
      "\n",
      " After batch 2, the loss is    0.47.\n",
      "\n",
      " After batch 4, the loss is    0.43.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4506 - mae: 0.5415\n",
      "Epoch 67: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 69/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.5558 - mae: 0.5882\n",
      " After batch 0, the loss is    0.56.\n",
      "\n",
      " After batch 2, the loss is    0.43.\n",
      "\n",
      " After batch 4, the loss is    0.45.\n",
      "\n",
      " After batch 6, the loss is    0.44.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4586 - mae: 0.5384\n",
      "Epoch 68: Average loss is    0.44, mean absolute error is    0.53.\n",
      "Epoch 70/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3999 - mae: 0.5106\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.40.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.40.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3913 - mae: 0.5086\n",
      "Epoch 69: Average loss is    0.40, mean absolute error is    0.51.\n",
      "Epoch 71/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3739 - mae: 0.4750\n",
      " After batch 0, the loss is    0.37.\n",
      "\n",
      " After batch 2, the loss is    0.38.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3862 - mae: 0.4873\n",
      "Epoch 70: Average loss is    0.39, mean absolute error is    0.49.\n",
      "Epoch 72/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4385 - mae: 0.5556\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.41.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4081 - mae: 0.5238\n",
      "Epoch 71: Average loss is    0.41, mean absolute error is    0.52.\n",
      "Epoch 73/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4331 - mae: 0.5437\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.41.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4023 - mae: 0.5065\n",
      "Epoch 72: Average loss is    0.38, mean absolute error is    0.48.\n",
      "Epoch 74/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3424 - mae: 0.4765\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.40.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3848 - mae: 0.4945\n",
      "Epoch 73: Average loss is    0.39, mean absolute error is    0.49.\n",
      "Epoch 75/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2638 - mae: 0.4211\n",
      " After batch 0, the loss is    0.26.\n",
      "\n",
      " After batch 2, the loss is    0.35.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3519 - mae: 0.4776\n",
      "Epoch 74: Average loss is    0.38, mean absolute error is    0.50.\n",
      "Epoch 76/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3161 - mae: 0.4403\n",
      " After batch 0, the loss is    0.32.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3810 - mae: 0.4964\n",
      "Epoch 75: Average loss is    0.39, mean absolute error is    0.51.\n",
      "Epoch 77/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3539 - mae: 0.4637\n",
      " After batch 0, the loss is    0.35.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3720 - mae: 0.4796\n",
      "Epoch 76: Average loss is    0.39, mean absolute error is    0.49.\n",
      "Epoch 78/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3180 - mae: 0.4556\n",
      " After batch 0, the loss is    0.32.\n",
      "\n",
      " After batch 2, the loss is    0.32.\n",
      "\n",
      " After batch 4, the loss is    0.37.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3465 - mae: 0.4752\n",
      "Epoch 77: Average loss is    0.38, mean absolute error is    0.49.\n",
      "Epoch 79/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2634 - mae: 0.4120\n",
      " After batch 0, the loss is    0.26.\n",
      "\n",
      " After batch 2, the loss is    0.36.\n",
      "\n",
      " After batch 4, the loss is    0.40.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3630 - mae: 0.4742\n",
      "Epoch 78: Average loss is    0.39, mean absolute error is    0.49.\n",
      "Epoch 80/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3358 - mae: 0.4484\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.37.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3657 - mae: 0.4835\n",
      "Epoch 79: Average loss is    0.37, mean absolute error is    0.49.\n",
      "Epoch 81/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3893 - mae: 0.5029\n",
      " After batch 0, the loss is    0.39.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.40.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3874 - mae: 0.4981\n",
      "Epoch 80: Average loss is    0.40, mean absolute error is    0.50.\n",
      "Epoch 82/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3953 - mae: 0.5080\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.42.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3976 - mae: 0.5041\n",
      "Epoch 81: Average loss is    0.41, mean absolute error is    0.51.\n",
      "Epoch 83/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3610 - mae: 0.4577\n",
      " After batch 0, the loss is    0.36.\n",
      "\n",
      " After batch 2, the loss is    0.36.\n",
      "\n",
      " After batch 4, the loss is    0.41.\n",
      "\n",
      " After batch 6, the loss is    0.41.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3935 - mae: 0.4927\n",
      "Epoch 82: Average loss is    0.41, mean absolute error is    0.51.\n",
      "Epoch 84/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3983 - mae: 0.5009\n",
      " After batch 0, the loss is    0.40.\n",
      "\n",
      " After batch 2, the loss is    0.37.\n",
      "\n",
      " After batch 4, the loss is    0.42.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3932 - mae: 0.4899\n",
      "Epoch 83: Average loss is    0.39, mean absolute error is    0.49.\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4096 - mae: 0.5143\n",
      " After batch 0, the loss is    0.41.\n",
      "\n",
      " After batch 2, the loss is    0.35.\n",
      "\n",
      " After batch 4, the loss is    0.37.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3777 - mae: 0.4828\n",
      "Epoch 84: Average loss is    0.38, mean absolute error is    0.49.\n",
      "Epoch 86/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3446 - mae: 0.4541\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.35.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3505 - mae: 0.4689\n",
      "Epoch 85: Average loss is    0.35, mean absolute error is    0.47.\n",
      "Epoch 87/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4274 - mae: 0.5536\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.45.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.39.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4066 - mae: 0.5159\n",
      "Epoch 86: Average loss is    0.39, mean absolute error is    0.50.\n",
      "Epoch 88/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4421 - mae: 0.5241\n",
      " After batch 0, the loss is    0.44.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3864 - mae: 0.4984\n",
      "Epoch 87: Average loss is    0.38, mean absolute error is    0.50.\n",
      "Epoch 89/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2753 - mae: 0.4315\n",
      " After batch 0, the loss is    0.28.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3750 - mae: 0.4832\n",
      "Epoch 88: Average loss is    0.38, mean absolute error is    0.48.\n",
      "Epoch 90/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3027 - mae: 0.4329\n",
      " After batch 0, the loss is    0.30.\n",
      "\n",
      " After batch 2, the loss is    0.38.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.36.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3605 - mae: 0.4716\n",
      "Epoch 89: Average loss is    0.36, mean absolute error is    0.47.\n",
      "Epoch 91/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3471 - mae: 0.4866\n",
      " After batch 0, the loss is    0.35.\n",
      "\n",
      " After batch 2, the loss is    0.36.\n",
      "\n",
      " After batch 4, the loss is    0.34.\n",
      "\n",
      " After batch 6, the loss is    0.34.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3495 - mae: 0.4774\n",
      "Epoch 90: Average loss is    0.34, mean absolute error is    0.47.\n",
      "Epoch 92/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.4263 - mae: 0.4863\n",
      " After batch 0, the loss is    0.43.\n",
      "\n",
      " After batch 2, the loss is    0.37.\n",
      "\n",
      " After batch 4, the loss is    0.38.\n",
      "\n",
      " After batch 6, the loss is    0.37.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3856 - mae: 0.4849\n",
      "Epoch 91: Average loss is    0.37, mean absolute error is    0.48.\n",
      "Epoch 93/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2592 - mae: 0.4131\n",
      " After batch 0, the loss is    0.26.\n",
      "\n",
      " After batch 2, the loss is    0.37.\n",
      "\n",
      " After batch 4, the loss is    0.35.\n",
      "\n",
      " After batch 6, the loss is    0.37.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3447 - mae: 0.4695\n",
      "Epoch 92: Average loss is    0.37, mean absolute error is    0.49.\n",
      "Epoch 94/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3201 - mae: 0.4429\n",
      " After batch 0, the loss is    0.32.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.35.\n",
      "\n",
      " After batch 6, the loss is    0.37.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3542 - mae: 0.4680\n",
      "Epoch 93: Average loss is    0.37, mean absolute error is    0.48.\n",
      "Epoch 95/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3572 - mae: 0.4935\n",
      " After batch 0, the loss is    0.36.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.37.\n",
      "\n",
      " After batch 6, the loss is    0.36.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3582 - mae: 0.4877\n",
      "Epoch 94: Average loss is    0.36, mean absolute error is    0.49.\n",
      "Epoch 96/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3208 - mae: 0.4672\n",
      " After batch 0, the loss is    0.32.\n",
      "\n",
      " After batch 2, the loss is    0.33.\n",
      "\n",
      " After batch 4, the loss is    0.36.\n",
      "\n",
      " After batch 6, the loss is    0.37.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3533 - mae: 0.4744\n",
      "Epoch 95: Average loss is    0.37, mean absolute error is    0.48.\n",
      "Epoch 97/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2359 - mae: 0.3945\n",
      " After batch 0, the loss is    0.24.\n",
      "\n",
      " After batch 2, the loss is    0.33.\n",
      "\n",
      " After batch 4, the loss is    0.34.\n",
      "\n",
      " After batch 6, the loss is    0.34.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3214 - mae: 0.4579\n",
      "Epoch 96: Average loss is    0.34, mean absolute error is    0.47.\n",
      "Epoch 98/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3425 - mae: 0.4821\n",
      " After batch 0, the loss is    0.34.\n",
      "\n",
      " After batch 2, the loss is    0.38.\n",
      "\n",
      " After batch 4, the loss is    0.35.\n",
      "\n",
      " After batch 6, the loss is    0.34.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3488 - mae: 0.4797\n",
      "Epoch 97: Average loss is    0.34, mean absolute error is    0.47.\n",
      "Epoch 99/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.3895 - mae: 0.5042\n",
      " After batch 0, the loss is    0.39.\n",
      "\n",
      " After batch 2, the loss is    0.39.\n",
      "\n",
      " After batch 4, the loss is    0.39.\n",
      "\n",
      " After batch 6, the loss is    0.38.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3876 - mae: 0.4941\n",
      "Epoch 98: Average loss is    0.38, mean absolute error is    0.49.\n",
      "Epoch 100/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.2831 - mae: 0.4220\n",
      " After batch 0, the loss is    0.28.\n",
      "\n",
      " After batch 2, the loss is    0.34.\n",
      "\n",
      " After batch 4, the loss is    0.34.\n",
      "\n",
      " After batch 6, the loss is    0.35.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3405 - mae: 0.4552\n",
      "Epoch 99: Average loss is    0.35, mean absolute error is    0.46.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa1421cc50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(train_data, train_targets, epochs=100,\n",
    "         batch_size=64, callbacks=[LossAndMetricCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is    0.90.\n",
      "\n",
      " After batch 1, the loss is    0.93.\n",
      "2/2 - 0s - loss: 0.9341 - mae: 0.7627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.934086799621582, 0.7626619338989258]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_data, test_targets, verbose=2, callbacks=[LossAndMetricCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "2/2 - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.613745  ],\n",
       "       [-0.7648672 ],\n",
       "       [-0.9248775 ],\n",
       "       [ 0.33509323],\n",
       "       [-0.4790194 ],\n",
       "       [-0.7748435 ],\n",
       "       [ 0.32043898],\n",
       "       [ 0.5458553 ],\n",
       "       [ 0.15522107],\n",
       "       [-0.7155451 ],\n",
       "       [-0.67694384],\n",
       "       [-0.8393685 ],\n",
       "       [-1.0530607 ],\n",
       "       [-0.975545  ],\n",
       "       [-0.51548874],\n",
       "       [-1.0388272 ],\n",
       "       [-0.74339646],\n",
       "       [ 0.23138644],\n",
       "       [-0.617802  ],\n",
       "       [-1.3198166 ],\n",
       "       [-0.25708285],\n",
       "       [ 0.41513258],\n",
       "       [-0.90968305],\n",
       "       [-0.74967945],\n",
       "       [-0.57438093],\n",
       "       [-0.49649426],\n",
       "       [ 1.1550509 ],\n",
       "       [-0.89632416],\n",
       "       [-0.4414843 ],\n",
       "       [-0.5638791 ],\n",
       "       [ 0.2081291 ],\n",
       "       [ 0.1818149 ],\n",
       "       [-0.7027743 ],\n",
       "       [ 0.3858471 ],\n",
       "       [ 0.05537266],\n",
       "       [-0.6231588 ],\n",
       "       [-1.0836759 ],\n",
       "       [ 0.1430587 ],\n",
       "       [ 0.28019285],\n",
       "       [-0.44112054],\n",
       "       [-0.6331344 ],\n",
       "       [ 0.4631385 ],\n",
       "       [-0.6090166 ],\n",
       "       [-0.11809668],\n",
       "       [-1.2221885 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "model.predict(test_data, verbose=2, callbacks=[LossAndMetricCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Callback - Learning Rate Scheduler\n",
    "\n",
    "The learning rate of the model will change as a function of the epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tuples that contain the epoch number and new learning rate\n",
    "lr_schedule = [(4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)]\n",
    "\n",
    "# Define an auxillary function that returns the new learning rate based on the epoch number\n",
    "def get_new_lr(epoch, lr):\n",
    "    # Check if the epoch number exists in the defined schedule\n",
    "    sched_epoch = [i for i in range(len(lr_schedule)) if lr_schedule[i][0] == int(epoch)]\n",
    "    if len(sched_epoch) > 0:\n",
    "        return lr_schedule[sched_epoch[0]][1]\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate scheduler callback\n",
    "class LRScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, new_lr_fn):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        self.new_lr_fn = new_lr_fn\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Get the current learning rate\n",
    "        curr_lr = self.model.optimizer.lr.numpy()\n",
    "        \n",
    "        # Call the auxillary learning rate function\n",
    "        new_lr = self.new_lr_fn(epoch, curr_lr)\n",
    "        \n",
    "        # Set the new learning rate as the current learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "        print(\"Learning rate for epoch {} is {:7.3f}\".format(epoch, new_lr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Learning rate for epoch 0 is   0.001\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3365 - mae: 0.4682\n",
      "Epoch 2/100\n",
      "Learning rate for epoch 1 is   0.001\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3556 - mae: 0.4721\n",
      "Epoch 3/100\n",
      "Learning rate for epoch 2 is   0.001\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3439 - mae: 0.4696\n",
      "Epoch 4/100\n",
      "Learning rate for epoch 3 is   0.001\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3679 - mae: 0.4863\n",
      "Epoch 5/100\n",
      "Learning rate for epoch 4 is   0.030\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5575 - mae: 0.5896\n",
      "Epoch 6/100\n",
      "Learning rate for epoch 5 is   0.030\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6890 - mae: 0.6805\n",
      "Epoch 7/100\n",
      "Learning rate for epoch 6 is   0.030\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8408 - mae: 0.7373\n",
      "Epoch 8/100\n",
      "Learning rate for epoch 7 is   0.020\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8523 - mae: 0.7330\n",
      "Epoch 9/100\n",
      "Learning rate for epoch 8 is   0.020\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5805 - mae: 0.6133\n",
      "Epoch 10/100\n",
      "Learning rate for epoch 9 is   0.020\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6245 - mae: 0.6418\n",
      "Epoch 11/100\n",
      "Learning rate for epoch 10 is   0.020\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6048 - mae: 0.6236\n",
      "Epoch 12/100\n",
      "Learning rate for epoch 11 is   0.005\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6115 - mae: 0.6187\n",
      "Epoch 13/100\n",
      "Learning rate for epoch 12 is   0.005\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5584 - mae: 0.6104\n",
      "Epoch 14/100\n",
      "Learning rate for epoch 13 is   0.005\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5440 - mae: 0.5999\n",
      "Epoch 15/100\n",
      "Learning rate for epoch 14 is   0.005\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5104 - mae: 0.5757\n",
      "Epoch 16/100\n",
      "Learning rate for epoch 15 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5041 - mae: 0.5860\n",
      "Epoch 17/100\n",
      "Learning rate for epoch 16 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5006 - mae: 0.5670\n",
      "Epoch 18/100\n",
      "Learning rate for epoch 17 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4873 - mae: 0.5586\n",
      "Epoch 19/100\n",
      "Learning rate for epoch 18 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5001 - mae: 0.5709\n",
      "Epoch 20/100\n",
      "Learning rate for epoch 19 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4791 - mae: 0.5760\n",
      "Epoch 21/100\n",
      "Learning rate for epoch 20 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5210 - mae: 0.5756\n",
      "Epoch 22/100\n",
      "Learning rate for epoch 21 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5034 - mae: 0.5808\n",
      "Epoch 23/100\n",
      "Learning rate for epoch 22 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4517 - mae: 0.5429\n",
      "Epoch 24/100\n",
      "Learning rate for epoch 23 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4852 - mae: 0.5608\n",
      "Epoch 25/100\n",
      "Learning rate for epoch 24 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4686 - mae: 0.5555\n",
      "Epoch 26/100\n",
      "Learning rate for epoch 25 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4992 - mae: 0.5800\n",
      "Epoch 27/100\n",
      "Learning rate for epoch 26 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4492 - mae: 0.5473\n",
      "Epoch 28/100\n",
      "Learning rate for epoch 27 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4500 - mae: 0.5322\n",
      "Epoch 29/100\n",
      "Learning rate for epoch 28 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4752 - mae: 0.5565\n",
      "Epoch 30/100\n",
      "Learning rate for epoch 29 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4651 - mae: 0.5519\n",
      "Epoch 31/100\n",
      "Learning rate for epoch 30 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4258 - mae: 0.5162\n",
      "Epoch 32/100\n",
      "Learning rate for epoch 31 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4567 - mae: 0.5459\n",
      "Epoch 33/100\n",
      "Learning rate for epoch 32 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4778 - mae: 0.5765\n",
      "Epoch 34/100\n",
      "Learning rate for epoch 33 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4350 - mae: 0.5311\n",
      "Epoch 35/100\n",
      "Learning rate for epoch 34 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4435 - mae: 0.5391\n",
      "Epoch 36/100\n",
      "Learning rate for epoch 35 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5441 - mae: 0.6129\n",
      "Epoch 37/100\n",
      "Learning rate for epoch 36 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4925 - mae: 0.5660\n",
      "Epoch 38/100\n",
      "Learning rate for epoch 37 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4492 - mae: 0.5329\n",
      "Epoch 39/100\n",
      "Learning rate for epoch 38 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4595 - mae: 0.5337\n",
      "Epoch 40/100\n",
      "Learning rate for epoch 39 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4392 - mae: 0.5341\n",
      "Epoch 41/100\n",
      "Learning rate for epoch 40 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4727 - mae: 0.5408\n",
      "Epoch 42/100\n",
      "Learning rate for epoch 41 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4566 - mae: 0.5423\n",
      "Epoch 43/100\n",
      "Learning rate for epoch 42 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4211 - mae: 0.5205\n",
      "Epoch 44/100\n",
      "Learning rate for epoch 43 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4642 - mae: 0.5490\n",
      "Epoch 45/100\n",
      "Learning rate for epoch 44 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4559 - mae: 0.5386\n",
      "Epoch 46/100\n",
      "Learning rate for epoch 45 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4055 - mae: 0.5098\n",
      "Epoch 47/100\n",
      "Learning rate for epoch 46 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4396 - mae: 0.5308\n",
      "Epoch 48/100\n",
      "Learning rate for epoch 47 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4348 - mae: 0.5310\n",
      "Epoch 49/100\n",
      "Learning rate for epoch 48 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4042 - mae: 0.5161\n",
      "Epoch 50/100\n",
      "Learning rate for epoch 49 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4283 - mae: 0.5131\n",
      "Epoch 51/100\n",
      "Learning rate for epoch 50 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4590 - mae: 0.5514\n",
      "Epoch 52/100\n",
      "Learning rate for epoch 51 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4476 - mae: 0.5348\n",
      "Epoch 53/100\n",
      "Learning rate for epoch 52 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3891 - mae: 0.4928\n",
      "Epoch 54/100\n",
      "Learning rate for epoch 53 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4094 - mae: 0.5217\n",
      "Epoch 55/100\n",
      "Learning rate for epoch 54 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4178 - mae: 0.5211\n",
      "Epoch 56/100\n",
      "Learning rate for epoch 55 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3911 - mae: 0.4939\n",
      "Epoch 57/100\n",
      "Learning rate for epoch 56 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4018 - mae: 0.5036\n",
      "Epoch 58/100\n",
      "Learning rate for epoch 57 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4259 - mae: 0.5322\n",
      "Epoch 59/100\n",
      "Learning rate for epoch 58 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4231 - mae: 0.5135\n",
      "Epoch 60/100\n",
      "Learning rate for epoch 59 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4456 - mae: 0.5388\n",
      "Epoch 61/100\n",
      "Learning rate for epoch 60 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4194 - mae: 0.5154\n",
      "Epoch 62/100\n",
      "Learning rate for epoch 61 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4487 - mae: 0.5441\n",
      "Epoch 63/100\n",
      "Learning rate for epoch 62 is   0.007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4283 - mae: 0.5348\n",
      "Epoch 64/100\n",
      "Learning rate for epoch 63 is   0.007\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3755 - mae: 0.4967\n",
      "Epoch 65/100\n",
      "Learning rate for epoch 64 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3957 - mae: 0.5067\n",
      "Epoch 66/100\n",
      "Learning rate for epoch 65 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4476 - mae: 0.5258\n",
      "Epoch 67/100\n",
      "Learning rate for epoch 66 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4160 - mae: 0.5152\n",
      "Epoch 68/100\n",
      "Learning rate for epoch 67 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4137 - mae: 0.5279\n",
      "Epoch 69/100\n",
      "Learning rate for epoch 68 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4568 - mae: 0.5457\n",
      "Epoch 70/100\n",
      "Learning rate for epoch 69 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4012 - mae: 0.5080\n",
      "Epoch 71/100\n",
      "Learning rate for epoch 70 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4203 - mae: 0.5062\n",
      "Epoch 72/100\n",
      "Learning rate for epoch 71 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4136 - mae: 0.5064\n",
      "Epoch 73/100\n",
      "Learning rate for epoch 72 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3992 - mae: 0.5074\n",
      "Epoch 74/100\n",
      "Learning rate for epoch 73 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3803 - mae: 0.4982\n",
      "Epoch 75/100\n",
      "Learning rate for epoch 74 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3749 - mae: 0.4896\n",
      "Epoch 76/100\n",
      "Learning rate for epoch 75 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3844 - mae: 0.4841\n",
      "Epoch 77/100\n",
      "Learning rate for epoch 76 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4014 - mae: 0.4940\n",
      "Epoch 78/100\n",
      "Learning rate for epoch 77 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3604 - mae: 0.4798\n",
      "Epoch 79/100\n",
      "Learning rate for epoch 78 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4341 - mae: 0.5314\n",
      "Epoch 80/100\n",
      "Learning rate for epoch 79 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4338 - mae: 0.5334\n",
      "Epoch 81/100\n",
      "Learning rate for epoch 80 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3700 - mae: 0.4843\n",
      "Epoch 82/100\n",
      "Learning rate for epoch 81 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3583 - mae: 0.4791\n",
      "Epoch 83/100\n",
      "Learning rate for epoch 82 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3981 - mae: 0.5071\n",
      "Epoch 84/100\n",
      "Learning rate for epoch 83 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3694 - mae: 0.4924\n",
      "Epoch 85/100\n",
      "Learning rate for epoch 84 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3864 - mae: 0.5074\n",
      "Epoch 86/100\n",
      "Learning rate for epoch 85 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3696 - mae: 0.4917\n",
      "Epoch 87/100\n",
      "Learning rate for epoch 86 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3871 - mae: 0.4955\n",
      "Epoch 88/100\n",
      "Learning rate for epoch 87 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3927 - mae: 0.5056\n",
      "Epoch 89/100\n",
      "Learning rate for epoch 88 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3710 - mae: 0.4926\n",
      "Epoch 90/100\n",
      "Learning rate for epoch 89 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3771 - mae: 0.4933\n",
      "Epoch 91/100\n",
      "Learning rate for epoch 90 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3624 - mae: 0.4848\n",
      "Epoch 92/100\n",
      "Learning rate for epoch 91 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3908 - mae: 0.4937\n",
      "Epoch 93/100\n",
      "Learning rate for epoch 92 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3725 - mae: 0.4844\n",
      "Epoch 94/100\n",
      "Learning rate for epoch 93 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4025 - mae: 0.5092\n",
      "Epoch 95/100\n",
      "Learning rate for epoch 94 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3816 - mae: 0.4978\n",
      "Epoch 96/100\n",
      "Learning rate for epoch 95 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3916 - mae: 0.5000\n",
      "Epoch 97/100\n",
      "Learning rate for epoch 96 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4193 - mae: 0.5230\n",
      "Epoch 98/100\n",
      "Learning rate for epoch 97 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3948 - mae: 0.5014\n",
      "Epoch 99/100\n",
      "Learning rate for epoch 98 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3851 - mae: 0.4876\n",
      "Epoch 100/100\n",
      "Learning rate for epoch 99 is   0.007\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3529 - mae: 0.4685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9e1a0e898>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_data, train_targets, epochs=100, batch_size=64,\n",
    "         callbacks=[LRScheduler(get_new_lr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a learning rate scheduler callback provided by the Tensorflow library. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule function\n",
    "def lr_sched(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "7/7 [==============================] - 1s 3ms/step - loss: 1603737129014263808.0000 - mae: 949102528.0000\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1649106912966344704.0000 - mae: 960255200.0000\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1599191524606738432.0000 - mae: 945897232.0000\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1611664229893406720.0000 - mae: 950902296.0000\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1594268186415464448.0000 - mae: 946361200.0000\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1623476334850211840.0000 - mae: 953421720.0000\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1621329864814493696.0000 - mae: 952740944.0000\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1595099777983315968.0000 - mae: 945529352.0000\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1596245623718281216.0000 - mae: 946172424.0000\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1611375367572946944.0000 - mae: 951395032.0000\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025999998673796654.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1570416807091634176.0000 - mae: 940260776.0000\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.03699999867379665.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1611879923151011840.0000 - mae: 949875808.0000\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.03700000047683716.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1609563458309718016.0000 - mae: 949652248.0000\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.050000000476837156.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1580014959486566400.0000 - mae: 942941896.0000\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.05000000074505806.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1613662248679505920.0000 - mae: 950036408.0000\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.06500000074505806.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1570393064512421888.0000 - mae: 941166136.0000\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.06499999761581421.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1624065192046362624.0000 - mae: 954896704.0000\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.08199999761581421.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1595694029658390528.0000 - mae: 946540088.0000\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0819999948143959.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1594521091269722112.0000 - mae: 945478016.0000\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.10099999481439591.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1624135732589232128.0000 - mae: 954969376.0000\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.10099999606609344.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1642479022514372608.0000 - mae: 960280720.0000\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.12199999606609345.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1642788620936937472.0000 - mae: 959608224.0000\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.12199999392032623.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1588922807557816320.0000 - mae: 944237192.0000\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.14499999392032623.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1603365047407476736.0000 - mae: 947677824.0000\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.14499999582767487.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1545107269190942720.0000 - mae: 931697880.0000\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.16999999582767486.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1606474620909649920.0000 - mae: 947975656.0000\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.17000000178813934.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1573074910811521024.0000 - mae: 939803616.0000\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.19700000178813934.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1582952081461870592.0000 - mae: 942901112.0000\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.19699999690055847.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1611837660672819200.0000 - mae: 948945368.0000\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.22599999690055847.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1579847283963330560.0000 - mae: 941119504.0000\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.22599999606609344.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1601582189303037952.0000 - mae: 949438200.0000\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.2569999960660935.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1565007416041406464.0000 - mae: 936684248.0000\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.25699999928474426.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1591087007218532352.0000 - mae: 942329568.0000\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.2899999992847443.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1566668760931106816.0000 - mae: 938157840.0000\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.28999999165534973.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1535113275430141952.0000 - mae: 929490968.0000\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.32499999165534976.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1538143048439955456.0000 - mae: 929188344.0000\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.32499998807907104.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1574979196231352320.0000 - mae: 938034048.0000\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.361999988079071.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1513159240858992640.0000 - mae: 923063240.0000\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.3619999885559082.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1551516958124408832.0000 - mae: 933375744.0000\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.4009999885559082.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1514612262654967808.0000 - mae: 922665672.0000\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.4009999930858612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 1523584105738928128.0000 - mae: 926592488.0000\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.4419999930858612.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1532260042756063232.0000 - mae: 927020120.0000\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.44200000166893005.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1500737525424062464.0000 - mae: 919132896.0000\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.48500000166893004.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1471951675353726976.0000 - mae: 909076672.0000\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.48500001430511475.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1507809927811301376.0000 - mae: 920940880.0000\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.5300000143051148.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1505144728805441536.0000 - mae: 919530616.0000\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.5300000309944153.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1452060049718378496.0000 - mae: 903790176.0000\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.5770000309944153.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1451923967974572032.0000 - mae: 905204536.0000\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.5770000219345093.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1471974988436209664.0000 - mae: 908770936.0000\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.6260000219345093.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1478604785853661184.0000 - mae: 912635160.0000\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.6260000467300415.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1430902869019590656.0000 - mae: 899712688.0000\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.6770000467300415.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1441971446417981440.0000 - mae: 899765832.0000\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.6770000457763672.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1427180712921923584.0000 - mae: 898892848.0000\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.7300000457763672.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1387990304577028096.0000 - mae: 888967560.0000\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.7300000190734863.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1409829113765560320.0000 - mae: 894918040.0000\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.7850000190734864.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1343295191267672064.0000 - mae: 879595480.0000\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.7850000262260437.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1336264673401503744.0000 - mae: 882545432.0000\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.8420000262260438.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1303165112797888512.0000 - mae: 878964256.0000\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.8420000076293945.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1235054387546554368.0000 - mae: 877358760.0000\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.9010000076293945.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1116810570633314304.0000 - mae: 865129704.0000\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.9010000228881836.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1022451023903457280.0000 - mae: 869217792.0000\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.9620000228881835.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 901049581393936384.0000 - mae: 859647696.0000\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.9620000123977661.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 834331885835386880.0000 - mae: 861997120.0000\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.025000012397766.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 799773728568246272.0000 - mae: 858093888.0000\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.024999976158142.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 779551802767966208.0000 - mae: 856971848.0000\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.089999976158142.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 765860924497068032.0000 - mae: 849358184.0000\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 1.090000033378601.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 763276831653625856.0000 - mae: 850097504.0000\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.157000033378601.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 751372264640872448.0000 - mae: 845772376.0000\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.1570000648498535.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 747604049313923072.0000 - mae: 844272776.0000\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.2260000648498535.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 726880703261704192.0000 - mae: 832308640.0000\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 1.2260000705718994.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 715913667480125440.0000 - mae: 827224264.0000\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 1.2970000705718994.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 710315693005864960.0000 - mae: 824751592.0000\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 1.2970000505447388.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 711821491359973376.0000 - mae: 824127808.0000\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 1.3700000505447387.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 705293768135278592.0000 - mae: 820657416.0000\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 1.3700000047683716.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 689493399497080832.0000 - mae: 812361600.0000\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 1.4450000047683715.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 687115757141622784.0000 - mae: 811454424.0000\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 1.4450000524520874.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 679460639361466368.0000 - mae: 806086632.0000\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 1.5220000524520874.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 671398702279032832.0000 - mae: 800994784.0000\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 1.5220000743865967.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 671982757701746688.0000 - mae: 802861144.0000\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 1.6010000743865966.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 657729144225792000.0000 - mae: 793009824.0000\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 1.6010000705718994.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 643940117362245632.0000 - mae: 785934336.0000\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 1.6820000705718994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 631616267052122112.0000 - mae: 779655808.0000\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 1.6820000410079956.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 628574064406953984.0000 - mae: 775973592.0000\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.7650000410079956.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 623370505369419776.0000 - mae: 773692936.0000\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 1.7649999856948853.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 608922759371685888.0000 - mae: 764622280.0000\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 1.8499999856948852.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 611112599987159040.0000 - mae: 766196760.0000\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.850000023841858.\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 594538432861044736.0000 - mae: 755047816.0000\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.9370000238418579.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 586923155197526016.0000 - mae: 751853408.0000\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 1.937000036239624.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 574619216355786752.0000 - mae: 744014104.0000\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 2.026000036239624.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 576158360835981312.0000 - mae: 743933752.0000\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 2.0260000228881836.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 562018473799057408.0000 - mae: 734684456.0000\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 2.117000022888184.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 556131980601720832.0000 - mae: 731243592.0000\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 2.117000102996826.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 548773451788189696.0000 - mae: 724866552.0000\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 2.210000102996826.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 533094592069763072.0000 - mae: 715959304.0000\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 2.2100000381469727.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 525277111640915968.0000 - mae: 710817720.0000\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 2.305000038146973.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 514422445088702464.0000 - mae: 703009368.0000\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 2.305000066757202.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 516114048023003136.0000 - mae: 703343488.0000\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 2.402000066757202.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 491769267151175680.0000 - mae: 689552440.0000\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 2.4019999504089355.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 499063659218075648.0000 - mae: 692687160.0000\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 2.5009999504089357.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 479946940172206080.0000 - mae: 679887232.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9e1ea8668>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_data, train_targets, epochs=100, batch_size=64,\n",
    "         callbacks=[LearningRateScheduler(lr_sched, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
